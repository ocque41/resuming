# AWS S3 and DynamoDB Setup Guide for Document Agent

## Introduction
This guide provides step-by-step instructions for setting up AWS S3 (for document storage) and DynamoDB (for metadata storage) for the AI Document Agent application. This guide is designed for beginners with no prior AWS experience.

## Prerequisites
- AWS account with administrative access
- AWS Management Console access
- Basic understanding of document storage concepts

## Part 1: Setting Up Amazon S3 for Document Storage

### Step 1: Creating an S3 Bucket
1. Sign in to the AWS Management Console at https://aws.amazon.com/
2. In the search bar at the top, type "S3" and select the "S3" service
3. Click the "Create bucket" button
4. Enter bucket details:
   - Bucket name: `ai-document-agent-storage` (must be globally unique, consider adding your organization name)
   - AWS Region: Select the region closest to your users (e.g., us-east-1)
   - Object Ownership: Select "ACLs disabled"
   - Block Public Access settings: Keep all checkboxes selected (recommended for security)
   - Bucket Versioning: Enable (allows you to recover previous versions of documents)
   - Default encryption: Enable with Amazon S3 managed keys (SSE-S3)
5. Click "Create bucket"

### Step 2: Creating Folder Structure
1. Click on your newly created bucket name in the S3 console
2. Click "Create folder" button
3. Create the following folders:
   - Name: `documents` (for storing uploaded documents)
   - Name: `temp` (for temporary processing files)
   - Name: `exports` (for exported document files)
4. For each folder, click "Create folder"

### Step 3: Setting Up Lifecycle Rules
1. In your bucket, click the "Management" tab
2. Scroll down to "Lifecycle rules" and click "Create lifecycle rule"
3. Enter rule details:
   - Lifecycle rule name: `TempFileCleanup`
   - Choose a rule scope: Apply to all objects in the bucket
   - Filter type: Prefix
   - Prefix: `temp/`
4. Under "Lifecycle rule actions":
   - Check "Expire current versions of objects"
   - Set "Days after object creation" to 1 (or your preferred retention period)
5. Click "Create rule"

### Step 4: Setting Up CORS for S3 Bucket
1. In your bucket, click the "Permissions" tab
2. Scroll down to "Cross-origin resource sharing (CORS)"
3. Click "Edit"
4. Copy and paste the following CORS configuration:
```json
[
    {
        "AllowedHeaders": [
            "*"
        ],
        "AllowedMethods": [
            "GET",
            "PUT",
            "POST",
            "DELETE",
            "HEAD"
        ],
        "AllowedOrigins": [
            "http://localhost:3000",
            "https://your-production-domain.com"
        ],
        "ExposeHeaders": [
            "ETag",
            "Content-Length",
            "Content-Type"
        ],
        "MaxAgeSeconds": 3000
    }
]
```
5. Replace "https://your-production-domain.com" with your actual production domain
6. Click "Save changes"

## Part 2: Setting Up Amazon DynamoDB for Metadata Storage

### Step 1: Creating a DynamoDB Table for Documents
1. In the AWS Management Console, search for "DynamoDB" and select it
2. Click "Create table" button
3. Enter table details:
   - Table name: `DocumentMetadata`
   - Partition key: `document_id` (type: String)
   - Sort key: Leave empty
4. Table settings:
   - Default settings: Keep selected
   - Or customize settings:
     * Read/write capacity mode: On-demand
     * Encryption at rest: Default (owned by Amazon DynamoDB)
5. Click "Create table"

### Step 2: Creating a DynamoDB Table for User Documents
1. Click "Create table" button again
2. Enter table details:
   - Table name: `UserDocuments`
   - Partition key: `user_id` (type: String)
   - Sort key: `document_id` (type: String)
3. Keep default settings or customize as before
4. Click "Create table"

### Step 3: Creating Global Secondary Indexes (Optional)
1. Select the `UserDocuments` table
2. Click the "Indexes" tab
3. Click "Create index" button
4. Enter index details:
   - Partition key: `document_id` (type: String)
   - Sort key: Leave empty
   - Index name: `DocumentIdIndex`
   - Projected attributes: All
5. Click "Create index"

### Step 4: Add Sample Document Metadata (For Testing)
1. Select the `DocumentMetadata` table
2. Click the "Actions" dropdown and select "Create item"
3. Enter the following attribute values:
   - document_id: "sample-doc-001"
   - Click "Add new attribute" and select "String"
   - Attribute name: "title"
   - Value: "Sample Document"
   - Click "Add new attribute" again, select "String"
   - Attribute name: "file_type"
   - Value: "pdf"
   - Add more attributes as needed (created_at, updated_at, status, etc.)
4. Click "Create item"

## Part 3: Setting Up IAM Policies

### Step 1: Creating IAM Policy for S3 Access
1. In the AWS Management Console, search for "IAM" and select it
2. In the left navigation, click "Policies"
3. Click "Create policy" button
4. Click the "JSON" tab
5. Replace the default policy with:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:PutObject",
                "s3:DeleteObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::ai-document-agent-storage",
                "arn:aws:s3:::ai-document-agent-storage/*"
            ]
        }
    ]
}
```
6. Replace "ai-document-agent-storage" with your actual bucket name
7. Click "Next"
8. Name the policy: `AIDocumentAgentS3Policy`
9. Add description: "Policy for AI Document Agent to access S3 bucket"
10. Click "Create policy"

### Step 2: Creating IAM Policy for DynamoDB Access
1. Click "Create policy" button again
2. Click the "JSON" tab
3. Replace the default policy with:
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "dynamodb:GetItem",
                "dynamodb:PutItem",
                "dynamodb:UpdateItem",
                "dynamodb:DeleteItem",
                "dynamodb:Query",
                "dynamodb:Scan"
            ],
            "Resource": [
                "arn:aws:dynamodb:*:*:table/DocumentMetadata",
                "arn:aws:dynamodb:*:*:table/UserDocuments",
                "arn:aws:dynamodb:*:*:table/UserDocuments/index/*"
            ]
        }
    ]
}
```
4. Click "Next"
5. Name the policy: `AIDocumentAgentDynamoDBPolicy`
6. Add description: "Policy for AI Document Agent to access DynamoDB tables"
7. Click "Create policy"

### Step 3: Attaching Policies to Lambda Role
1. In the IAM console, click "Roles" in the left navigation
2. Find and click on your Lambda execution role (created in the Lambda setup)
3. Click "Add permissions" and select "Attach policies"
4. Search for and select:
   - `AIDocumentAgentS3Policy`
   - `AIDocumentAgentDynamoDBPolicy`
5. Click "Attach policies"

## Part 4: Testing S3 and DynamoDB Setup

### Step 1: Testing S3 Access
1. Create a simple text file on your computer with some test content
2. Go to the S3 console and navigate to your bucket
3. Click the "Upload" button
4. Select your test file and click "Upload"
5. Verify the file appears in your bucket

### Step 2: Testing DynamoDB Access
1. Go to the DynamoDB console
2. Select the `DocumentMetadata` table
3. Click the "Items" tab
4. Verify you can see your sample document
5. Try creating another item for testing

## Part 5: Integrating with Python Backend

### Step 1: Updating Environment Variables
Add the following variables to your Lambda environment:
```
AWS_S3_BUCKET=ai-document-agent-storage
AWS_REGION=us-east-1
```

### Step 2: S3 Integration Code
Here's an example of how to interact with S3 from your Python code:

```python
import boto3
import os
from botocore.exceptions import ClientError

def upload_file_to_s3(file_path, object_name=None):
    """Upload a file to an S3 bucket
    
    :param file_path: File to upload
    :param object_name: S3 object name (if None, file_path is used)
    :return: True if file was uploaded, else False
    """
    # If S3 object_name not specified, use file_path
    if object_name is None:
        object_name = os.path.basename(file_path)
    
    # Get bucket name from environment variable
    bucket = os.getenv('AWS_S3_BUCKET')
    
    # Upload the file
    s3_client = boto3.client('s3')
    try:
        s3_client.upload_file(file_path, bucket, object_name)
    except ClientError as e:
        print(f"Error uploading file to S3: {e}")
        return False
    return True

def get_file_from_s3(object_name, file_path):
    """Download a file from S3 bucket
    
    :param object_name: S3 object name
    :param file_path: Local file path to save to
    :return: True if file was downloaded, else False
    """
    # Get bucket name from environment variable
    bucket = os.getenv('AWS_S3_BUCKET')
    
    # Download the file
    s3_client = boto3.client('s3')
    try:
        s3_client.download_file(bucket, object_name, file_path)
    except ClientError as e:
        print(f"Error downloading file from S3: {e}")
        return False
    return True
```

### Step 3: DynamoDB Integration Code
Here's an example of how to interact with DynamoDB from your Python code:

```python
import boto3
import uuid
from datetime import datetime
from boto3.dynamodb.conditions import Key

def create_document_metadata(title, file_type, file_size, user_id):
    """Create new document metadata in DynamoDB
    
    :param title: Document title
    :param file_type: File type (pdf, docx, etc)
    :param file_size: File size in bytes
    :param user_id: User ID
    :return: document_id if successful, else None
    """
    # Create a DynamoDB resource
    dynamodb = boto3.resource('dynamodb')
    
    # Get the tables
    document_table = dynamodb.Table('DocumentMetadata')
    user_document_table = dynamodb.Table('UserDocuments')
    
    # Generate unique document ID
    document_id = str(uuid.uuid4())
    timestamp = datetime.now().isoformat()
    
    # Create document metadata
    try:
        document_table.put_item(
            Item={
                'document_id': document_id,
                'title': title,
                'file_type': file_type,
                'file_size': file_size,
                'created_at': timestamp,
                'updated_at': timestamp,
                'status': 'active'
            }
        )
        
        # Create user-document relationship
        user_document_table.put_item(
            Item={
                'user_id': user_id,
                'document_id': document_id,
                'created_at': timestamp
            }
        )
        
        return document_id
    except Exception as e:
        print(f"Error creating document metadata: {e}")
        return None

def get_document_metadata(document_id):
    """Get document metadata from DynamoDB
    
    :param document_id: Document ID
    :return: Document metadata if found, else None
    """
    # Create a DynamoDB resource
    dynamodb = boto3.resource('dynamodb')
    
    # Get the table
    document_table = dynamodb.Table('DocumentMetadata')
    
    try:
        response = document_table.get_item(
            Key={
                'document_id': document_id
            }
        )
        
        # Check if item exists
        if 'Item' in response:
            return response['Item']
        else:
            print(f"Document {document_id} not found")
            return None
    except Exception as e:
        print(f"Error getting document metadata: {e}")
        return None

def get_user_documents(user_id):
    """Get all documents for a user
    
    :param user_id: User ID
    :return: List of document IDs
    """
    # Create a DynamoDB resource
    dynamodb = boto3.resource('dynamodb')
    
    # Get the table
    user_document_table = dynamodb.Table('UserDocuments')
    
    try:
        response = user_document_table.query(
            KeyConditionExpression=Key('user_id').eq(user_id)
        )
        
        # Return list of document IDs
        return [item['document_id'] for item in response.get('Items', [])]
    except Exception as e:
        print(f"Error getting user documents: {e}")
        return []
```

## Part 6: Securing Your S3 and DynamoDB Resources

### Step 1: Enabling Server-Side Encryption for DynamoDB
1. Go to the DynamoDB console
2. Select one of your tables (e.g., `DocumentMetadata`)
3. Click the "Additional settings" tab
4. Under "Encryption at rest", click "Edit"
5. Select "Owned by Amazon DynamoDB" or "AWS KMS" (for higher security)
6. Click "Save changes"
7. Repeat for other tables

### Step 2: Setting Up S3 Access Logging (Optional)
1. Go to the S3 console and select your bucket
2. Click the "Properties" tab
3. Scroll down to "Server access logging" and click "Edit"
4. Enable server access logging
5. Select a target bucket (create one if needed)
6. Set a target prefix (e.g., "logs/")
7. Click "Save changes"

### Step 3: Implementing Backup Strategy
1. Go to the AWS Backup console (search for "Backup" in AWS Management Console)
2. Click "Create backup plan"
3. Choose "Build a new plan"
4. Enter plan details:
   - Name: `DocumentAgentBackupPlan`
   - Rule configuration:
     * Rule name: `DailyBackup`
     * Backup frequency: Daily
     * Backup window: Start time (e.g., 12:00 AM UTC)
     * Retention period: 30 days (or as needed)
5. Click "Create plan"
6. On the backup plan page, click "Assign resources"
7. Select resource types:
   - Check "DynamoDB" and "S3"
8. Under "Assign by", select "Resource ID"
9. Select your DynamoDB tables and S3 bucket
10. Click "Assign resources"

## Common Issues and Solutions

### S3 Access Denied Errors
If you see access denied errors when accessing S3:
1. Check your IAM policy to ensure it has the correct permissions
2. Verify the bucket name in your code matches the actual bucket name
3. Check if the bucket policy restricts access

### DynamoDB Throughput Exceeded
If you see throughput exceeded errors:
1. Consider switching to on-demand capacity mode
2. If using provisioned capacity, increase read/write capacity units

### S3 CORS Issues
If you have CORS issues with S3:
1. Verify your CORS configuration includes your frontend domain
2. Ensure the HTTP methods you're using are allowed
3. Check that the necessary headers are allowed

## Next Steps
1. Implement regular backups for both S3 and DynamoDB
2. Set up CloudWatch alarms for monitoring storage and database usage
3. Implement versioning and lifecycle policies for document management
4. Consider using S3 Intelligent-Tiering for cost optimization

## Reference
- [Amazon S3 Documentation](https://docs.aws.amazon.com/s3/)
- [Amazon DynamoDB Documentation](https://docs.aws.amazon.com/dynamodb/)
- [AWS SDK for Python (Boto3) Documentation](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) 